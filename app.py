import streamlit as st
import os
from datetime import datetime
import time
import base64
from fact_checker import FactChecker
import auth
import db_utils
from pdf_export import generate_fact_check_pdf
from model_manager import model_manager

from reportlab.pdfgen import canvas
from io import BytesIO


def generate_test_pdf():
    buffer = BytesIO()
    c = canvas.Canvas(buffer)
    c.drawString(100, 750, "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•PDF")
    c.save()
    buffer.seek(0)
    return buffer.getvalue()


# åˆå§‹åŒ–æ•°æ®åº“
db_utils.init_db()

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="AIè™šå‡æ–°é—»æ£€æµ‹å™¨",
    page_icon="ğŸ”",
    layout="wide",
    menu_items={"Get Help": None, "Report a bug": None, "About": None},
)


def check_user_config_status():
    """æ£€æŸ¥ç”¨æˆ·é…ç½®çŠ¶æ€ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦æ˜¾ç¤ºé…ç½®å‘å¯¼"""
    from user_config import get_user_config_manager
    
    config_manager = get_user_config_manager()
    if not config_manager:
        return False  # æœªç™»å½•ï¼Œä¸éœ€è¦æ£€æŸ¥é…ç½®
    
    user_config = config_manager.get_user_config()
    
    # æ£€æŸ¥æ˜¯å¦æœ‰åŸºæœ¬é…ç½®
    has_model_config = bool(user_config.get("model_config", {}))
    has_working_config = "config_completed" in user_config
    
    return has_model_config and has_working_config

def show_initial_config_wizard():
    """æ˜¾ç¤ºåˆå§‹é…ç½®å‘å¯¼"""
    st.title("ğŸš€ æ¬¢è¿ä½¿ç”¨AIè™šå‡æ–°é—»æ£€æµ‹å™¨")
    st.markdown("""
    åœ¨å¼€å§‹ä½¿ç”¨å‰ï¼Œè¯·å…ˆè¿›è¡Œä¸€æ¬¡æ€§é…ç½®ã€‚
    é…ç½®å®Œæˆåï¼Œæ‚¨å°±å¯ä»¥ç›´æ¥ä½¿ç”¨ç³»ç»Ÿäº†ã€‚
    """)
    
    st.divider()
    
    # è‡ªåŠ¨æ£€æµ‹é…ç½®
    st.subheader("ğŸ” æ­¥éª¤1: æ£€æµ‹æœ¬åœ°ç¯å¢ƒ")
    
    auto_config = detect_available_services()
    if auto_config:
        st.success(f"âœ… æ£€æµ‹åˆ°å¯ç”¨æœåŠ¡: {auto_config['name']}")
        st.info(f"ğŸ“ æœåŠ¡åœ°å€: {auto_config['url']}")
        st.info(f"ğŸ¤– å¯ç”¨æ¨¡å‹: {len(auto_config['available_models'])}ä¸ª")
        
        # æ˜¾ç¤ºæ¨¡å‹é€‰æ‹©
        st.subheader("ğŸ¤– é€‰æ‹©æ¨¡å‹")
        
        # åˆ†ç±»æ¨¡å‹
        chat_models, embedding_models = categorize_models(auto_config['available_models'])
        
        col1, col2 = st.columns(2)
        with col1:
            if chat_models:
                selected_chat_model = st.selectbox(
                    "ğŸ’¬ èŠå¤©æ¨¡å‹",
                    options=chat_models,
                    help=f"å…±{len(chat_models)}ä¸ªèŠå¤©æ¨¡å‹å¯ç”¨"
                )
            else:
                st.warning("æœªæ‰¾åˆ°èŠå¤©æ¨¡å‹")
                selected_chat_model = None
        
        with col2:
            if embedding_models:
                selected_embedding_model = st.selectbox(
                    "ğŸ§  åµŒå…¥æ¨¡å‹",
                    options=embedding_models,
                    help=f"å…±{len(embedding_models)}ä¸ªåµŒå…¥æ¨¡å‹å¯ç”¨"
                )
            else:
                # å¦‚æœæ²¡æœ‰åµŒå…¥æ¨¡å‹ï¼Œä»èŠå¤©æ¨¡å‹ä¸­é€‰æ‹©ä¸€ä¸ª
                if chat_models:
                    selected_embedding_model = st.selectbox(
                        "ğŸ§  åµŒå…¥æ¨¡å‹",
                        options=chat_models,
                        help="æœªæ‰¾åˆ°ä¸“ç”¨åµŒå…¥æ¨¡å‹ï¼Œä½¿ç”¨èŠå¤©æ¨¡å‹ä»£æ›¿"
                    )
                else:
                    st.warning("æœªæ‰¾åˆ°å¯ç”¨æ¨¡å‹")
                    selected_embedding_model = None
        
        if selected_chat_model and selected_embedding_model:
            # æ·»åŠ æœç´¢å¼•æ“é€‰æ‹©
            st.subheader("ğŸ” é€‰æ‹©æœç´¢å¼•æ“")
            search_options = {
                "ğŸ¦† DuckDuckGo (æ¨è)": "duckduckgo",
                "ğŸ” SearXNG (æœ¬åœ°)": "searxng"
            }
            
            selected_search = st.radio(
                "æœç´¢å¼•æ“",
                options=list(search_options.keys()),
                help="DuckDuckGo æ— éœ€é…ç½®ï¼ŒSearXNG éœ€è¦æœ¬åœ°éƒ¨ç½²",
                horizontal=True
            )
            
            search_provider = search_options[selected_search]
            searxng_url = None
            
            # å¦‚æœé€‰æ‹©äº† SearXNGï¼Œè®©ç”¨æˆ·é…ç½®åœ°å€
            if search_provider == "searxng":
                searxng_url = st.text_input(
                    "ğŸŒ SearXNG æœåŠ¡åœ°å€",
                    value="http://localhost:8090",
                    help="è¯·è¾“å…¥æ‚¨çš„ SearXNG å®ä¾‹åœ°å€",
                    placeholder="http://localhost:8090"
                )
                
                if searxng_url:
                    # æµ‹è¯• SearXNG è¿æ¥
                    searxng_available = test_searxng_connection(searxng_url)
                    if searxng_available:
                        st.success("âœ… SearXNG æœåŠ¡å¯ç”¨")
                    else:
                        st.warning("âš ï¸ SearXNG æœåŠ¡ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥åœ°å€æˆ–æœåŠ¡çŠ¶æ€")
            
            if st.button("âœ¨ ä½¿ç”¨æ­¤é…ç½®", type="primary", use_container_width=True):
                auto_config['chat_model'] = selected_chat_model
                auto_config['embedding_model'] = selected_embedding_model
                auto_config['search_provider'] = search_provider
                if searxng_url:
                    auto_config['searxng_url'] = searxng_url
                save_auto_config(auto_config)
                st.success("âœ… é…ç½®å®Œæˆï¼æ­£åœ¨è¿›å…¥ä¸»ç•Œé¢...")
                time.sleep(1)
                st.rerun()
                return
    else:
        st.warning("âš ï¸ æœªæ£€æµ‹åˆ°æœ¬åœ°AIæœåŠ¡ï¼Œè¯·æ‰‹åŠ¨é…ç½®")
    
    st.divider()
    
    # æ‰‹åŠ¨é…ç½®
    st.subheader("âš™ï¸ æ­¥éª¤2: æ‰‹åŠ¨é…ç½®")
    
    # ç®€åŒ–çš„é…ç½®é€‰é¡¹
    config_option = st.radio(
        "é€‰æ‹©AIæœåŠ¡ç±»å‹",
        options=[
            "ğŸš€ Ollama (æœ¬åœ°æ¨è)",
            "ğŸ’» LM Studio (æœ¬åœ°å›¾å½¢ç•Œé¢)", 
            "â˜ï¸ OpenAI (äº‘ç«¯æœåŠ¡)",
            "ğŸ”§ è‡ªå®šä¹‰é…ç½®"
        ],
        help="é€‰æ‹©æ‚¨è¦ä½¿ç”¨çš„AIæœåŠ¡ç±»å‹"
    )
    
    manual_config = None
    
    if "ğŸš€ Ollama" in config_option:
        st.subheader("ğŸš€ Ollama é…ç½®")
        models = get_models_for_provider("ollama", "http://localhost:11434")
        if models:
            chat_models, embedding_models = categorize_models(models)
            
            col1, col2 = st.columns(2)
            with col1:
                chat_model = st.selectbox("ğŸ’¬ èŠå¤©æ¨¡å‹", options=chat_models if chat_models else models)
            with col2:
                embedding_model = st.selectbox("ğŸ§  åµŒå…¥æ¨¡å‹", options=embedding_models if embedding_models else models)
            
            if chat_model and embedding_model:
                # æ·»åŠ æœç´¢å¼•æ“é€‰æ‹©
                st.subheader("ğŸ” é€‰æ‹©æœç´¢å¼•æ“")
                search_options = {
                    "ğŸ¦† DuckDuckGo (æ¨è)": "duckduckgo",
                    "ğŸ” SearXNG (æœ¬åœ°)": "searxng"
                }
                
                selected_search = st.radio(
                    "æœç´¢å¼•æ“",
                    options=list(search_options.keys()),
                    help="DuckDuckGo æ— éœ€é…ç½®ï¼ŒSearXNG éœ€è¦æœ¬åœ°éƒ¨ç½²",
                    horizontal=True,
                    key="ollama_search"
                )
                
                search_provider = search_options[selected_search]
                searxng_url = None
                
                # å¦‚æœé€‰æ‹©äº† SearXNGï¼Œè®©ç”¨æˆ·é…ç½®åœ°å€
                if search_provider == "searxng":
                    searxng_url = st.text_input(
                        "ğŸŒ SearXNG æœåŠ¡åœ°å€",
                        value="http://localhost:8090",
                        help="è¯·è¾“å…¥æ‚¨çš„ SearXNG å®ä¾‹åœ°å€",
                        placeholder="http://localhost:8090",
                        key="ollama_searxng_url"
                    )
                
                manual_config = {
                    "name": "Ollama",
                    "provider": "ollama",
                    "url": "http://localhost:11434/v1",
                    "chat_model": chat_model,
                    "embedding_model": embedding_model,
                    "search_provider": search_provider
                }
                
                if searxng_url:
                    manual_config["searxng_url"] = searxng_url
        else:
            st.warning("âš ï¸ æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡ï¼Œè¯·ç¡®ä¿ Ollama å·²å¯åŠ¨")
    
    elif "ğŸ’» LM Studio" in config_option:
        st.subheader("ğŸ’» LM Studio é…ç½®")
        models = get_models_for_provider("lmstudio", "http://localhost:1234")
        if models:
            chat_models, embedding_models = categorize_models(models)
            
            col1, col2 = st.columns(2)
            with col1:
                chat_model = st.selectbox("ğŸ’¬ èŠå¤©æ¨¡å‹", options=chat_models if chat_models else models)
            with col2:
                embedding_model = st.selectbox("ğŸ§  åµŒå…¥æ¨¡å‹", options=embedding_models if embedding_models else models)
            
            if chat_model and embedding_model:
                # æ·»åŠ æœç´¢å¼•æ“é€‰æ‹©
                st.subheader("ğŸ” é€‰æ‹©æœç´¢å¼•æ“")
                search_options = {
                    "ğŸ¦† DuckDuckGo (æ¨è)": "duckduckgo",
                    "ğŸ” SearXNG (æœ¬åœ°)": "searxng"
                }
                
                selected_search = st.radio(
                    "æœç´¢å¼•æ“",
                    options=list(search_options.keys()),
                    help="DuckDuckGo æ— éœ€é…ç½®ï¼ŒSearXNG éœ€è¦æœ¬åœ°éƒ¨ç½²",
                    horizontal=True,
                    key="lmstudio_search"
                )
                
                search_provider = search_options[selected_search]
                searxng_url = None
                
                # å¦‚æœé€‰æ‹©äº† SearXNGï¼Œè®©ç”¨æˆ·é…ç½®åœ°å€
                if search_provider == "searxng":
                    searxng_url = st.text_input(
                        "ğŸŒ SearXNG æœåŠ¡åœ°å€",
                        value="http://localhost:8090",
                        help="è¯·è¾“å…¥æ‚¨çš„ SearXNG å®ä¾‹åœ°å€",
                        placeholder="http://localhost:8090",
                        key="lmstudio_searxng_url"
                    )
                
                manual_config = {
                    "name": "LM Studio", 
                    "provider": "lmstudio",
                    "url": "http://localhost:1234/v1",
                    "chat_model": chat_model,
                    "embedding_model": embedding_model,
                    "search_provider": search_provider
                }
                
                if searxng_url:
                    manual_config["searxng_url"] = searxng_url
        else:
            st.warning("âš ï¸ æ— æ³•è¿æ¥åˆ° LM Studio æœåŠ¡ï¼Œè¯·ç¡®ä¿ LM Studio å·²å¯åŠ¨")
    
    elif "â˜ï¸ OpenAI" in config_option:
        st.subheader("â˜ï¸ OpenAI é…ç½®")
        api_key = st.text_input("ğŸ”‘ OpenAI API Key", type="password", help="è¯·è¾“å…¥æ‚¨çš„OpenAI APIå¯†é’¥")
        if api_key:
            # é¢„å®šä¹‰ OpenAI æ¨¡å‹ï¼ˆå› ä¸ºéœ€è¦ API Key æ‰èƒ½è·å–ï¼‰
            openai_models = {
                "ğŸ’¬ èŠå¤©æ¨¡å‹": ["gpt-4o", "gpt-4o-mini", "gpt-4-turbo", "gpt-4", "gpt-3.5-turbo"],
                "ğŸ§  åµŒå…¥æ¨¡å‹": ["text-embedding-3-large", "text-embedding-3-small", "text-embedding-ada-002"]
            }
            
            col1, col2 = st.columns(2)
            with col1:
                chat_model = st.selectbox("ğŸ’¬ èŠå¤©æ¨¡å‹", options=openai_models["ğŸ’¬ èŠå¤©æ¨¡å‹"])
            with col2:
                embedding_model = st.selectbox("ğŸ§  åµŒå…¥æ¨¡å‹", options=openai_models["ğŸ§  åµŒå…¥æ¨¡å‹"])
            
            # æ·»åŠ æœç´¢å¼•æ“é€‰æ‹©
            st.subheader("ğŸ” é€‰æ‹©æœç´¢å¼•æ“")
            search_options = {
                "ğŸ¦† DuckDuckGo (æ¨è)": "duckduckgo",
                "ğŸ” SearXNG (æœ¬åœ°)": "searxng"
            }
            
            selected_search = st.radio(
                "æœç´¢å¼•æ“",
                options=list(search_options.keys()),
                help="DuckDuckGo æ— éœ€é…ç½®ï¼ŒSearXNG éœ€è¦æœ¬åœ°éƒ¨ç½²",
                horizontal=True,
                key="openai_search"
            )
            
            search_provider = search_options[selected_search]
            searxng_url = None
            
            # å¦‚æœé€‰æ‹©äº† SearXNGï¼Œè®©ç”¨æˆ·é…ç½®åœ°å€
            if search_provider == "searxng":
                searxng_url = st.text_input(
                    "ğŸŒ SearXNG æœåŠ¡åœ°å€",
                    value="http://localhost:8090",
                    help="è¯·è¾“å…¥æ‚¨çš„ SearXNG å®ä¾‹åœ°å€",
                    placeholder="http://localhost:8090",
                    key="openai_searxng_url"
                )
            
            manual_config = {
                "name": "OpenAI",
                "provider": "openai", 
                "url": "https://api.openai.com/v1",
                "api_key": api_key,
                "chat_model": chat_model,
                "embedding_model": embedding_model,
                "search_provider": search_provider
            }
            
            if searxng_url:
                manual_config["searxng_url"] = searxng_url
    
    elif "ğŸ”§ è‡ªå®šä¹‰" in config_option:
        with st.expander("ğŸš€ è‡ªå®šä¹‰é…ç½®", expanded=True):
            url = st.text_input("ğŸŒ APIåœ°å€", placeholder="http://localhost:8000/v1")
            
            if url:
                # å°è¯•è·å–æ¨¡å‹åˆ—è¡¨
                models = get_models_for_provider("custom", url.rstrip('/v1'))
                
                if models:
                    st.success(f"âœ… æ£€æµ‹åˆ° {len(models)} ä¸ªå¯ç”¨æ¨¡å‹")
                    chat_models, embedding_models = categorize_models(models)
                    
                    col1, col2 = st.columns(2)
                    with col1:
                        chat_model = st.selectbox("ğŸ’¬ èŠå¤©æ¨¡å‹", options=chat_models if chat_models else models)
                    with col2:
                        embedding_model = st.selectbox("ğŸ§  åµŒå…¥æ¨¡å‹", options=embedding_models if embedding_models else models)
                    
                    if chat_model and embedding_model:
                        # æ·»åŠ æœç´¢å¼•æ“é€‰æ‹©
                        st.subheader("ğŸ” é€‰æ‹©æœç´¢å¼•æ“")
                        search_options = {
                            "ğŸ¦† DuckDuckGo (æ¨è)": "duckduckgo",
                            "ğŸ” SearXNG (æœ¬åœ°)": "searxng"
                        }
                        
                        selected_search = st.radio(
                            "æœç´¢å¼•æ“",
                            options=list(search_options.keys()),
                            help="DuckDuckGo æ— éœ€é…ç½®ï¼ŒSearXNG éœ€è¦æœ¬åœ°éƒ¨ç½²",
                            horizontal=True,
                            key="custom_search_1"
                        )
                        
                        search_provider = search_options[selected_search]
                        searxng_url = None
                        
                        # å¦‚æœé€‰æ‹©äº† SearXNGï¼Œè®©ç”¨æˆ·é…ç½®åœ°å€
                        if search_provider == "searxng":
                            searxng_url = st.text_input(
                                "ğŸŒ SearXNG æœåŠ¡åœ°å€",
                                value="http://localhost:8090",
                                help="è¯·è¾“å…¥æ‚¨çš„ SearXNG å®ä¾‹åœ°å€",
                                placeholder="http://localhost:8090",
                                key="custom_searxng_url_1"
                            )
                        
                        manual_config = {
                            "name": "è‡ªå®šä¹‰é…ç½®",
                            "provider": "custom",
                            "url": url,
                            "chat_model": chat_model,
                            "embedding_model": embedding_model,
                            "search_provider": search_provider
                        }
                        
                        if searxng_url:
                            manual_config["searxng_url"] = searxng_url
                else:
                    st.warning("âš ï¸ æ— æ³•ä»æ­¤åœ°å€è·å–æ¨¡å‹åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥åœ°å€æ˜¯å¦æ­£ç¡®")
                    # æ‰‹åŠ¨è¾“å…¥æ¨¡å‹å
                    st.info("ğŸ“ è¯·æ‰‹åŠ¨è¾“å…¥æ¨¡å‹åç§°")
                    col1, col2 = st.columns(2)
                    with col1:
                        chat_model = st.text_input("ğŸ’¬ èŠå¤©æ¨¡å‹", placeholder="ä¾‹å¦‚: llama2")
                    with col2:
                        embedding_model = st.text_input("ğŸ§  åµŒå…¥æ¨¡å‹", placeholder="ä¾‹å¦‚: nomic-embed-text")
                    
                    if chat_model and embedding_model:
                        # æ·»åŠ æœç´¢å¼•æ“é€‰æ‹©
                        st.subheader("ğŸ” é€‰æ‹©æœç´¢å¼•æ“")
                        search_options = {
                            "ğŸ¦† DuckDuckGo (æ¨è)": "duckduckgo",
                            "ğŸ” SearXNG (æœ¬åœ°)": "searxng"
                        }
                        
                        selected_search = st.radio(
                            "æœç´¢å¼•æ“",
                            options=list(search_options.keys()),
                            help="DuckDuckGo æ— éœ€é…ç½®ï¼ŒSearXNG éœ€è¦æœ¬åœ°éƒ¨ç½²",
                            horizontal=True,
                            key="custom_search_2"
                        )
                        
                        search_provider = search_options[selected_search]
                        searxng_url = None
                        
                        # å¦‚æœé€‰æ‹©äº† SearXNGï¼Œè®©ç”¨æˆ·é…ç½®åœ°å€
                        if search_provider == "searxng":
                            searxng_url = st.text_input(
                                "ğŸŒ SearXNG æœåŠ¡åœ°å€",
                                value="http://localhost:8090",
                                help="è¯·è¾“å…¥æ‚¨çš„ SearXNG å®ä¾‹åœ°å€",
                                placeholder="http://localhost:8090",
                                key="custom_searxng_url_2"
                            )
                        
                        manual_config = {
                            "name": "è‡ªå®šä¹‰é…ç½®",
                            "provider": "custom",
                            "url": url,
                            "chat_model": chat_model,
                            "embedding_model": embedding_model,
                            "search_provider": search_provider
                        }
                        
                        if searxng_url:
                            manual_config["searxng_url"] = searxng_url
    
    # æµ‹è¯•é…ç½®
    if manual_config:
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("ğŸ”— æµ‹è¯•è¿æ¥", use_container_width=True):
                with st.spinner("æ­£åœ¨æµ‹è¯•è¿æ¥..."):
                    if test_config_connection(manual_config):
                        st.success("âœ… è¿æ¥æˆåŠŸï¼")
                    else:
                        st.error("âŒ è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
        
        with col2:
            if st.button("âœ¨ ä¿å­˜é…ç½®", type="primary", use_container_width=True):
                save_manual_config(manual_config)
                st.success("âœ… é…ç½®å®Œæˆï¼æ­£åœ¨è¿›å…¥ä¸»ç•Œé¢...")
                time.sleep(1)
                st.rerun()

def detect_available_services():
    """æ£€æµ‹å¯ç”¨çš„æœ¬åœ°æœåŠ¡å¹¶è·å–æ¨¡å‹åˆ—è¡¨"""
    import requests
    
    services = [
        ("http://localhost:11434", "Ollama", "ollama"),
        ("http://localhost:1234", "LM Studio", "lmstudio"),
        ("http://localhost:8000", "æœ¬åœ°API", "local_api")
    ]
    
    for url, name, provider in services:
        try:
            # å…ˆæµ‹è¯•åŸºæœ¬è¿æ¥
            response = requests.get(f"{url}/v1/models", timeout=3)
            if response.status_code == 200:
                # è·å–æ¨¡å‹åˆ—è¡¨
                models_data = response.json()
                available_models = []
                
                if "data" in models_data:
                    # OpenAIæ ¼å¼: {"data": [{"id": "model_name"}, ...]}
                    available_models = [model["id"] for model in models_data["data"]]
                elif "models" in models_data:
                    # Ollamaæ ¼å¼: {"models": [{"name": "model_name"}, ...]}
                    available_models = [model["name"] for model in models_data["models"]]
                elif isinstance(models_data, list):
                    # ç®€å•æ ¼å¼: ["model1", "model2", ...]
                    available_models = models_data
                
                if available_models:
                    return {
                        "name": name,
                        "provider": provider,
                        "url": f"{url}/v1",
                        "available_models": available_models
                    }
        except:
            continue
    return None

def categorize_models(models):
    """å°†æ¨¡å‹åˆ†ç±»ä¸ºèŠå¤©æ¨¡å‹å’ŒåµŒå…¥æ¨¡å‹"""
    chat_models = []
    embedding_models = []
    
    for model in models:
        model_lower = model.lower()
        # åˆ¤æ–­æ˜¯å¦ä¸ºåµŒå…¥æ¨¡å‹
        if any(keyword in model_lower for keyword in ['embed', 'embedding', 'nomic', 'bge', 'gte']):
            embedding_models.append(model)
        else:
            chat_models.append(model)
    
    return chat_models, embedding_models

def get_models_for_provider(provider_type, url):
    """ä¸ºæŒ‡å®šæä¾›å•†è·å–æ¨¡å‹åˆ—è¡¨"""
    import requests
    
    try:
        response = requests.get(f"{url}/models", timeout=5)
        if response.status_code == 200:
            models_data = response.json()
            
            if "data" in models_data:
                return [model["id"] for model in models_data["data"]]
            elif "models" in models_data:
                return [model["name"] for model in models_data["models"]]
            elif isinstance(models_data, list):
                return models_data
        return []
    except:
        return []

def test_searxng_connection(searxng_url="http://localhost:8090"):
    """æµ‹è¯• SearXNG è¿æ¥"""
    try:
        import requests
        # ç¡®ä¿ URLæ ¼å¼æ­£ç¡®
        if not searxng_url.startswith('http'):
            searxng_url = f"http://{searxng_url}"
        
        # æµ‹è¯•æœç´¢æ¥å£
        response = requests.get(f"{searxng_url}/search", 
                               params={"q": "test", "format": "json"}, 
                               timeout=3)
        return response.status_code == 200
    except:
        return False

def test_config_connection(config):
    """æµ‹è¯•é…ç½®è¿æ¥"""
    try:
        import requests
        response = requests.get(f"{config['url']}/models", timeout=3)
        return response.status_code == 200
    except:
        return False

def save_auto_config(config):
    """ä¿å­˜è‡ªåŠ¨æ£€æµ‹çš„é…ç½®"""
    from user_config import get_user_config_manager
    
    config_manager = get_user_config_manager()
    if config_manager:
        user_config = {
            "model_config": {
                "providers": {
                    config["provider"]: {
                        "base_url": config["url"]
                    }
                },
                "defaults": {
                    "llm_provider": config["provider"],
                    "llm_model": config["chat_model"],
                    "embedding_model": config["embedding_model"],
                    "search_provider": config.get("search_provider", "duckduckgo"),
                    "output_language": "zh"
                }
            },
            "config_completed": True,
            "config_source": "auto"
        }
        
        # å¦‚æœæœ‰è‡ªå®šä¹‰ SearXNG åœ°å€ï¼Œä¿å­˜åˆ°æœç´¢é…ç½®ä¸­
        if config.get("searxng_url"):
            user_config["search_config"] = {
                "search_providers": {
                    "searxng": {
                        "base_url": config["searxng_url"]
                    }
                }
            }
        
        config_manager.save_user_config(user_config)

def save_manual_config(config):
    """ä¿å­˜æ‰‹åŠ¨é…ç½®"""
    from user_config import get_user_config_manager
    
    config_manager = get_user_config_manager()
    if config_manager:
        user_config = {
            "model_config": {
                "providers": {
                    config["provider"]: {
                        "base_url": config["url"]
                    }
                },
                "defaults": {
                    "llm_provider": config["provider"],
                    "llm_model": config["chat_model"], 
                    "embedding_model": config["embedding_model"],
                    "search_provider": config.get("search_provider", "duckduckgo"),
                    "output_language": "zh"
                }
            },
            "config_completed": True,
            "config_source": "manual"
        }
        
        if "api_key" in config:
            user_config["model_config"]["providers"][config["provider"]]["api_key"] = config["api_key"]
        
        # å¦‚æœæœ‰è‡ªå®šä¹‰ SearXNG åœ°å€ï¼Œä¿å­˜åˆ°æœç´¢é…ç½®ä¸­
        if config.get("searxng_url"):
            user_config["search_config"] = {
                "search_providers": {
                    "searxng": {
                        "base_url": config["searxng_url"]
                    }
                }
            }
        
        config_manager.save_user_config(user_config)

def get_saved_config_info():
    """è·å–å·²ä¿å­˜çš„é…ç½®ä¿¡æ¯ç”¨äºæ˜¾ç¤º"""
    from user_config import get_user_config_manager
    
    config_manager = get_user_config_manager()
    if not config_manager:
        return None
    
    user_config = config_manager.get_user_config()
    model_config = user_config.get("model_config", {})
    defaults = model_config.get("defaults", {})
    
    return {
        "model_name": defaults.get("llm_model", "æœªé…ç½®"),
        "search_name": get_search_display_name(defaults.get("search_provider", "duckduckgo"))
    }

def get_search_display_name(search_provider):
    """è·å–æœç´¢å¼•æ“æ˜¾ç¤ºåç§°"""
    search_names = {
        "duckduckgo": "DuckDuckGo",
        "searxng": "SearXNG"
    }
    return search_names.get(search_provider, search_provider)

def get_config_parameters():
    """ä»å·²ä¿å­˜çš„é…ç½®è·å–å‚æ•°"""
    from user_config import get_user_config_manager
    
    config_manager = get_user_config_manager()
    if not config_manager:
        return None
    
    user_config = config_manager.get_user_config()
    model_config = user_config.get("model_config", {})
    
    if not model_config:
        return None
    
    providers = model_config.get("providers", {})
    defaults = model_config.get("defaults", {})
    
    provider_key = defaults.get("llm_provider")
    if not provider_key or provider_key not in providers:
        return None
    
    provider_config = providers[provider_key]
    
    return {
        "provider_key": provider_key,
        "api_base": provider_config.get("base_url"),
        "chat_model": defaults.get("llm_model"),
        "embedding_model": defaults.get("embedding_model"),
        "search_provider": defaults.get("search_provider", "duckduckgo"),
        "selected_language": defaults.get("output_language", "zh"),
        "provider_config": provider_config
    }

def reset_user_config():
    """é‡ç½®ç”¨æˆ·é…ç½®"""
    from user_config import get_user_config_manager
    
    config_manager = get_user_config_manager()
    if config_manager:
        config_manager.reset_config()

def show_simplified_fact_check_page():
    """æ˜¾ç¤ºç®€åŒ–çš„äº‹å®æ ¸æŸ¥é¡µé¢ - æ— å¤æ‚é…ç½®ç•Œé¢"""
    st.markdown(
        """
    æœ¬åº”ç”¨ç¨‹åºä½¿ç”¨æœ¬åœ°AIæ¨¡å‹éªŒè¯é™ˆè¿°çš„å‡†ç¡®æ€§ã€‚
    è¯·åœ¨ä¸‹æ–¹è¾“å…¥éœ€è¦æ ¸æŸ¥çš„æ–°é—»ï¼Œç³»ç»Ÿå°†æ£€ç´¢ç½‘ç»œè¯æ®è¿›è¡Œæ–°é—»æ ¸æŸ¥ã€‚
    """
    )

    # ç®€åŒ–çš„ä¾§è¾¹æ  - åªæ˜¾ç¤ºçŠ¶æ€å’ŒåŸºæœ¬ä¿¡æ¯
    with st.sidebar:
        st.header("ğŸ“Š ç³»ç»ŸçŠ¶æ€")
        
        # è·å–å·²ä¿å­˜çš„é…ç½®
        config_info = get_saved_config_info()
        if config_info:
            st.success(f"âœ… AIæ¨¡å‹: {config_info['model_name']}")
            st.success(f"âœ… æœç´¢å¼•æ“: {config_info['search_name']}")
        
        st.divider()
        
        # å¿«é€Ÿè®¾ç½® - åªæ˜¾ç¤ºå¿…è¦çš„
        with st.expander("âš™ï¸ å¿«é€Ÿè®¾ç½®"):
            temperature = st.slider(
                "åˆ›é€ æ€§",
                min_value=0.0,
                max_value=1.0,
                value=0.0,
                step=0.1,
                help="è¾ƒä½çš„å€¼ä½¿å“åº”æ›´ç¡®å®šï¼Œè¾ƒé«˜çš„å€¼ä½¿å“åº”æ›´å…·åˆ›é€ æ€§",
            )
            language = st.selectbox(
                "è¾“å‡ºè¯­è¨€",
                options=["è‡ªåŠ¨æ£€æµ‹", "ä¸­æ–‡", "English"],
                help="é€‰æ‹©AIå›å¤çš„è¯­è¨€"
            )
        
        st.divider()
        
        # é…ç½®ç®¡ç†é“¾æ¥
        if st.button("ğŸ”§ é‡æ–°é…ç½®", help="é‡æ–°è®¾ç½® AI æ¨¡å‹å’ŒæœåŠ¡"):
            reset_user_config()
            st.rerun()
        
        st.divider()
        st.markdown("### å…³äº")
        st.markdown("è™šå‡æ–°é—»æ£€æµ‹å™¨:")
        st.markdown("1. ä»æ–°é—»ä¸­æå–æ ¸å¿ƒå£°æ˜")
        st.markdown("2. åœ¨ç½‘ç»œä¸Šæœç´¢è¯æ®")
        st.markdown("3. ä½¿ç”¨BGE-M3æŒ‰ç›¸å…³æ€§å¯¹è¯æ®è¿›è¡Œæ’å")
        st.markdown("4. åŸºäºè¯æ®æä¾›ç»“è®º")
        st.markdown("ä½¿ç”¨Streamlitã€BGE-M3å’ŒLLMå¼€å‘ â¤ï¸")

    # ä½¿ç”¨å·²ä¿å­˜çš„é…ç½®è·å–å‚æ•°
    config_params = get_config_parameters()
    if not config_params:
        st.error("é…ç½®è·å–å¤±è´¥ï¼Œè¯·é‡æ–°é…ç½®")
        if st.button("é‡æ–°é…ç½®"):
            reset_user_config()
            st.rerun()
        return

    # ä»¥ä¸‹çš„é€»è¾‘ä¿æŒä¸å˜ï¼Œåªæ˜¯ä½¿ç”¨ä¿å­˜çš„é…ç½®å‚æ•°
    # å¦‚æœä¸å­˜åœ¨ï¼Œåˆå§‹åŒ–ä¼šè¯çŠ¶æ€ä»¥å­˜å‚¨èŠå¤©å†å²
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # æ˜¾ç¤ºèŠå¤©å†å²
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ä¸»è¾“å…¥åŒºåŸŸ
    user_input = st.chat_input("è¯·åœ¨ä¸‹æ–¹è¾“å…¥éœ€è¦æ ¸æŸ¥çš„æ–°é—»...")

    if user_input:
        # å°†ç”¨æˆ·æ¶ˆæ¯æ·»åŠ åˆ°èŠå¤©å†å²
        st.session_state.messages.append({"role": "user", "content": user_input})

        # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
        with st.chat_message("user"):
            st.markdown(user_input)

        # åˆ›å»ºåŠ©æ‰‹æ¶ˆæ¯å®¹å™¨ç”¨äºæµå¼è¾“å‡º
        assistant_message = st.chat_message("assistant")

        # åˆ›å»ºç©ºçš„placeholderç»„ä»¶ç”¨äºé€æ­¥æ›´æ–°
        claim_placeholder = assistant_message.empty()
        evidence_placeholder = assistant_message.empty()
        verdict_placeholder = assistant_message.empty()

        # æ£€æŸ¥æ¨¡å‹é…ç½®æ˜¯å¦æœ‰æ•ˆ - ä½¿ç”¨ä¿å­˜çš„é…ç½®
        api_base = config_params["api_base"]
        chat_model = config_params["chat_model"]
        embedding_model = config_params["embedding_model"]
        search_provider = config_params["search_provider"]
        selected_language = config_params["selected_language"]
        provider_config = config_params["provider_config"]
        
        if not api_base or not chat_model:
            st.error("é…ç½®ä¿¡æ¯ä¸å®Œæ•´ï¼Œè¯·é‡æ–°é…ç½®æ¨¡å‹æä¾›å•†")
            st.stop()

        if not embedding_model:
            st.error("é…ç½®ä¿¡æ¯ä¸å®Œæ•´ï¼Œè¯·é‡æ–°é…ç½®åµŒå…¥æ¨¡å‹")
            st.stop()

        # è·å–é…ç½®
        embedding_api_key = provider_config.get("api_key", "lm-studio")
        search_config = model_manager.get_search_provider_config(search_provider)
        searxng_url = search_config.get("base_url", "http://localhost:8090")
        
        # ä½¿ç”¨ä¾§è¾¹æ çš„è®¾ç½®è¦†ç›–é»˜è®¤å€¼
        max_tokens = 1000  # å›ºå®šå€¼ï¼Œç®€åŒ–é…ç½®

        # åˆå§‹åŒ–FactChecker
        fact_checker = FactChecker(
            api_base=api_base,
            model=chat_model,
            temperature=temperature,
            max_tokens=max_tokens,
            embedding_base_url=api_base,
            embedding_model=embedding_model,
            embedding_api_key=embedding_api_key,
            search_engine=search_provider,
            searxng_url=searxng_url,
            output_language=selected_language,
            search_config=search_config,
        )

        # ç¬¬1æ­¥ï¼šæå–å£°æ˜
        claim_placeholder.markdown("### ğŸ” æ­£åœ¨æå–æ–°é—»çš„æ ¸å¿ƒå£°æ˜...")
        claim = fact_checker.extract_claim(user_input)
        # å¤„ç†claimå­—ç¬¦ä¸²ï¼Œæå–"claim:"åé¢çš„å†…å®¹
        if "claim:" in claim.lower():
            claim = claim.split("claim:")[-1].strip()
        claim_placeholder.markdown(f"### ğŸ” æå–æ–°é—»çš„æ ¸å¿ƒå£°æ˜\n\n{claim}")

        # ç¬¬2æ­¥ï¼šæœç´¢è¯æ®
        evidence_placeholder.markdown("### ğŸŒ æ­£åœ¨æœç´¢ç›¸å…³è¯æ®...")
        # ä»é…ç½®ä¸­è·å–æœç´¢ç»“æœæ•°é‡
        search_max_results = search_config.get("max_results", 5)
        evidence_docs = fact_checker.search_evidence(claim, search_max_results)

        # ç¬¬3æ­¥ï¼šè·å–ç›¸å…³è¯æ®å—
        evidence_placeholder.markdown("### ğŸŒ æ­£åœ¨åˆ†æè¯æ®ç›¸å…³æ€§...")
        # åŠ¨æ€è®¡ç®—å±•ç¤ºçš„è¯æ®æ•°é‡ï¼šåŸºäºæœç´¢é…ç½® * è¯­è¨€æ•°é‡ * æ‰©å±•å€æ•°
        base_results = search_config.get("max_results", 5)
        language_count = 3  # ä¸­è‹±æ—¥ä¸‰ç§è¯­è¨€
        expansion_factor = (
            model_manager.get_current_config()
            .get("defaults", {})
            .get("evidence_display_multiplier", 2.0)
        )
        max_evidence_display = int(base_results * language_count * expansion_factor)

        evidence_chunks = fact_checker.get_evidence_chunks(
            evidence_docs, claim, top_k=max_evidence_display
        )

        # æ˜¾ç¤ºè¯æ®ç»“æœ
        evidence_md = "### ğŸ”— è¯æ®æ¥æº\n\n"
        # ä½¿ç”¨ç›¸åŒçš„è¯æ®å—è¿›è¡Œæ˜¾ç¤ºå’Œè¯„ä¼°
        evaluation_evidence = (
            evidence_chunks[:-1] if len(evidence_chunks) > 1 else evidence_chunks
        )

        for j, chunk in enumerate(evaluation_evidence):
            evidence_md += f"**[{j+1}]:**\n"
            evidence_md += f"{chunk['text']}\n"
            evidence_md += f"æ¥æº: {chunk['source']}\n\n"

        evidence_placeholder.markdown(evidence_md)

        # ç¬¬4æ­¥ï¼šè¯„ä¼°å£°æ˜
        verdict_placeholder.markdown("### âš–ï¸ æ­£åœ¨è¯„ä¼°å£°æ˜çœŸå®æ€§...")
        evaluation = fact_checker.evaluate_claim(claim, evaluation_evidence)

        # ç¡®å®šç»“è®ºè¡¨æƒ…ç¬¦å·
        verdict = evaluation["verdict"]
        if verdict.upper() == "TRUE":
            emoji = "âœ…"
            verdict_cn = "æ­£ç¡®"
        elif verdict.upper() == "FALSE":
            emoji = "âŒ"
            verdict_cn = "é”™è¯¯"
        elif verdict.upper() == "PARTIALLY TRUE":
            emoji = "âš ï¸"
            verdict_cn = "éƒ¨åˆ†æ­£ç¡®"
        else:
            emoji = "â“"
            verdict_cn = "æ— æ³•éªŒè¯"

        # æ˜¾ç¤ºæœ€ç»ˆç»“è®º
        verdict_md = f"### {emoji} ç»“è®º: {verdict_cn}\n\n"
        verdict_md += f"### æ¨ç†è¿‡ç¨‹\n\n{evaluation['reasoning']}\n\n"

        verdict_placeholder.markdown(verdict_md)

        # æ•´åˆå®Œæ•´çš„å“åº”å†…å®¹ç”¨äºä¿å­˜åˆ°èŠå¤©å†å²
        full_response = f"""
### ğŸ” æå–æ–°é—»çš„æ ¸å¿ƒå£°æ˜

{claim}

---

{evidence_md}

---

{verdict_md}
"""

        # æ·»åŠ åŠ©æ‰‹å“åº”åˆ°èŠå¤©å†å²
        st.session_state.messages.append(
            {"role": "assistant", "content": full_response}
        )

        # ä¿å­˜åˆ°æ•°æ®åº“
        db_utils.save_fact_check(
            st.session_state.user_id,
            user_input,
            claim,
            verdict,
            evaluation["reasoning"],
            evaluation_evidence,
        )


def show_history_page():
    """æ˜¾ç¤ºå†å²è®°å½•é¡µé¢"""
    st.header("å†å²è®°å½•")
    st.write("ä»¥ä¸‹æ˜¯æ‚¨è¿‡å»è¿›è¡Œçš„äº‹å®æ ¸æŸ¥è®°å½•")

    # åˆ†é¡µæ§åˆ¶
    items_per_page = 5
    total_items = db_utils.count_user_history(st.session_state.user_id)

    if "history_page" not in st.session_state:
        st.session_state.history_page = 0

    total_pages = (total_items + items_per_page - 1) // items_per_page

    if total_pages > 1:
        col1, col2, col3 = st.columns([1, 3, 1])
        with col1:
            if st.button("ä¸Šä¸€é¡µ", disabled=(st.session_state.history_page == 0)):
                st.session_state.history_page -= 1
                st.rerun()
        with col2:
            st.write(f"ç¬¬ {st.session_state.history_page + 1} é¡µï¼Œå…± {total_pages} é¡µ")
        with col3:
            if st.button(
                "ä¸‹ä¸€é¡µ",
                disabled=(
                    st.session_state.history_page == total_pages - 1 or total_pages == 0
                ),
            ):
                st.session_state.history_page += 1
                st.rerun()

    # è·å–ç”¨æˆ·å†å²è®°å½•
    history_items = db_utils.get_user_history(
        st.session_state.user_id,
        limit=items_per_page,
        offset=st.session_state.history_page * items_per_page,
    )

    if not history_items:
        st.info("æ‚¨è¿˜æ²¡æœ‰ä»»ä½•å†å²è®°å½•")
        return

    # æ˜¾ç¤ºå†å²è®°å½•
    for item in history_items:
        with st.container():
            cols = st.columns([4, 1, 1])
            with cols[0]:
                st.subheader(
                    f"{item['claim'][:100]}..."
                    if len(item["claim"]) > 100
                    else item["claim"]
                )

                # æ·»åŠ åˆ¤æ–­ç»“æœå’Œæ—¶é—´
                verdict = item["verdict"].upper()
                if verdict == "TRUE":
                    emoji = "âœ…"
                    verdict_cn = "æ­£ç¡®"
                elif verdict == "FALSE":
                    emoji = "âŒ"
                    verdict_cn = "é”™è¯¯"
                elif verdict == "PARTIALLY TRUE":
                    emoji = "âš ï¸"
                    verdict_cn = "éƒ¨åˆ†æ­£ç¡®"
                else:
                    emoji = "â“"
                    verdict_cn = "æ— æ³•éªŒè¯"

                st.write(f"ç»“è®º: {emoji} {verdict_cn}")
                st.write(f"æ—¶é—´: {item['created_at']}")

            with cols[1]:
                if st.button("æŸ¥çœ‹è¯¦æƒ…", key=f"view_{item['id']}"):
                    st.session_state.current_history_id = item["id"]
                    st.session_state.page = "details"
                    st.rerun()

            st.divider()


def show_history_detail_page():
    """æ˜¾ç¤ºå†å²è®°å½•è¯¦æƒ…é¡µé¢"""
    if st.session_state.current_history_id is None:
        st.error("æœªæ‰¾åˆ°å†å²è®°å½•")
        if st.button("è¿”å›å†å²åˆ—è¡¨"):
            st.session_state.page = "history"
            st.rerun()
        return

    # è·å–å†å²è®°å½•è¯¦æƒ…
    history_item = db_utils.get_history_by_id(st.session_state.current_history_id)

    if not history_item:
        st.error("æœªæ‰¾åˆ°å†å²è®°å½•")
        if st.button("è¿”å›å†å²åˆ—è¡¨"):
            st.session_state.page = "history"
            st.rerun()
        return

    # æ˜¾ç¤ºè¿”å›æŒ‰é’®
    if st.button("è¿”å›å†å²åˆ—è¡¨"):
        st.session_state.page = "history"
        st.rerun()

    # æ˜¾ç¤ºå†å²è®°å½•è¯¦æƒ…
    st.header("æ ¸æŸ¥è¯¦æƒ…")

    st.subheader("åŸå§‹æ–‡æœ¬")
    st.write(history_item["original_text"])

    st.subheader("ğŸ” æå–çš„æ ¸å¿ƒå£°æ˜")
    st.write(history_item["claim"])

    # æ˜¾ç¤ºè¯æ®
    st.subheader("ğŸ”— è¯æ®æ¥æº")
    for j, chunk in enumerate(history_item["evidence"]):
        st.markdown(f"**[{j+1}]:**")
        st.markdown(f"{chunk['text']}")
        st.markdown(f"æ¥æº: {chunk['source']}")
        if "similarity" in chunk and chunk["similarity"] is not None:
            st.markdown(f"ç›¸å…³æ€§: {chunk['similarity']:.2f}")
        st.markdown("---")

    # æ˜¾ç¤ºåˆ¤æ–­ç»“æœ
    verdict = history_item["verdict"].upper()
    if verdict == "TRUE":
        emoji = "âœ…"
        verdict_cn = "æ­£ç¡®"
    elif verdict == "FALSE":
        emoji = "âŒ"
        verdict_cn = "é”™è¯¯"
    elif verdict == "PARTIALLY TRUE":
        emoji = "âš ï¸"
        verdict_cn = "éƒ¨åˆ†æ­£ç¡®"
    else:
        emoji = "â“"
        verdict_cn = "æ— æ³•éªŒè¯"

    st.subheader(f"{emoji} ç»“è®º: {verdict_cn}")

    st.subheader("æ¨ç†è¿‡ç¨‹")
    st.write(history_item["reasoning"])

    # æ˜¾ç¤ºå¯¼å‡ºé€‰é¡¹
    st.divider()
    st.subheader("å¯¼å‡ºæŠ¥å‘Š")

    # åˆ›å»ºPDFå¯¼å‡ºæŒ‰é’®
    try:
        pdf_data = generate_fact_check_pdf(history_item)

        # ç”Ÿæˆæ–‡ä»¶å
        current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"äº‹å®æ ¸æŸ¥æŠ¥å‘Š_{current_time}.pdf"

        # ä½¿ç”¨HTMLå¼ºåˆ¶ä¸‹è½½
        pdf_b64 = base64.b64encode(pdf_data).decode()
        href = f"""
        <a href="data:application/pdf;base64,{pdf_b64}" 
        download="{filename}" 
        target="_blank"
        style="display: inline-block; padding: 0.25em 0.5em; 
        background-color: #4CAF50; color: white; 
        text-decoration: none; border-radius: 4px;">
        å¯¼å‡ºä¸ºPDF
        </a>
        """
        st.markdown(href, unsafe_allow_html=True)
    except Exception as e:
        st.error(f"PDFç”Ÿæˆé”™è¯¯: {str(e)}")
        st.info("è¯·ç¡®ä¿å·²å®‰è£…ReportLabåº“: pip install reportlab")


# å…¨å±€çŠ¶æ€åˆå§‹åŒ–
if "page" not in st.session_state:
    st.session_state.page = "home"  # å¯èƒ½çš„å€¼: 'home', 'history', 'details'

if "current_history_id" not in st.session_state:
    st.session_state.current_history_id = None

# æ—©æœŸæ£€æŸ¥æŒä¹…ç™»å½•çŠ¶æ€ - åœ¨ä»»ä½•UIæ˜¾ç¤ºä¹‹å‰
if "user_id" not in st.session_state or st.session_state.user_id is None:
    saved_login = auth.check_saved_login()
    if saved_login:
        st.session_state.user_id = saved_login["user_id"]
        st.session_state.username = saved_login["username"]
        st.session_state.persisted_login = saved_login

# æ£€æŸ¥æ˜¯å¦å·²ç™»å½•ï¼Œå¦åˆ™æ˜¾ç¤ºç™»å½•ç•Œé¢
is_authenticated = auth.show_auth_ui()

if is_authenticated:
    # ç”¨æˆ·å·²ç™»å½•ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦é…ç½®
    
    # æ£€æŸ¥ç”¨æˆ·é…ç½®çŠ¶æ€
    if not check_user_config_status():
        # æ˜¾ç¤ºé…ç½®å‘å¯¼
        show_initial_config_wizard()
    else:
        # é…ç½®å®Œæˆï¼Œæ˜¾ç¤ºä¸»åº”ç”¨ç¨‹åº
        # æ˜¾ç¤ºé¡¶éƒ¨å¯¼èˆªæ 
        col1, col2, col3, col4 = st.columns([2, 1, 1, 1])
        with col1:
            st.title("AIè™šå‡æ–°é—»æ£€æµ‹å™¨")
        with col2:
            if st.button("é¦–é¡µ", use_container_width=True):
                st.session_state.page = "home"
                st.rerun()
        with col3:
            if st.button("å†å²è®°å½•", use_container_width=True):
                st.session_state.page = "history"
                st.rerun()
        with col4:
            if st.button("ç™»å‡º", use_container_width=True):
                auth.logout()
                st.rerun()

        # æ˜¾ç¤ºå½“å‰ç”¨æˆ·ä¿¡æ¯
        st.write(f"å·²ç™»å½•ç”¨æˆ·: {st.session_state.username}")

        # æ ¹æ®å½“å‰é¡µé¢æ˜¾ç¤ºä¸åŒçš„å†…å®¹
        if st.session_state.page == "home":
            # ä¸»é¡µ - ä½¿ç”¨ç®€åŒ–çš„äº‹å®æ ¸æŸ¥ç•Œé¢
            show_simplified_fact_check_page()
        elif st.session_state.page == "history":
            # å†å²è®°å½•é¡µé¢
            show_history_page()
        elif st.session_state.page == "details":
            # å†å²è¯¦æƒ…é¡µé¢
            show_history_detail_page()
